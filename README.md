# ğŸ“ Predicting Student Dropout and Academic Success  
**Machine Learning Final Project**  
**Author:** Utsav Shakya  

---

## ğŸ“Œ Project Overview
This project applies **machine learning techniques** to predict student outcomes â€” **Dropout**, **Enrolled**, or **Graduate** â€” using real educational data from the **UCI Machine Learning Repository**.

The goal is to:
- Analyze student performance and socioeconomic factors
- Reduce dimensionality using **PCA**
- Train and compare multiple classification models
- Identify the most effective approach for early risk detection

---

## ğŸ§  Techniques Used
âœ” Data discovery & preprocessing  
âœ” Principal Component Analysis (PCA)  
âœ” Decision Tree analysis  
âœ” Multi-Layer Perceptron (MLP)  
âœ” Ensemble methods (AdaBoost & Gradient Boosting)  
âœ” Model evaluation with **Macro-F1** and **Stratified Cross-Validation**

---

## ğŸ“Š Dataset Information

| Attribute | Description |
|--------|------------|
| **Source** | [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success) |
| **Dataset Name** | Predict Studentsâ€™ Dropout and Academic Success |
| **Rows** | 4,424 |
| **Features** | 36 (academic, demographic, socioeconomic) |
| **Missing Values** | None |

### ğŸ¯ Target Classes
- **Dropout**
- **Enrolled**
- **Graduate**

---

## ğŸ” Exploratory Data Analysis (EDA)

**Key insights discovered:**
- ğŸ“ˆ Academic performance (1st & 2nd semester grades) had the **strongest correlation** with student outcomes.
- ğŸ’° Macroeconomic indicators (GDP, inflation) showed **very weak influence**.
- ğŸŒ³ A shallow decision tree revealed:
  - **2nd semester grades**
  - **Tuition fee status**  
  as major predictors of dropout risk.
- ğŸ“‰ PCA required **27 components to retain 96% variance**, indicating a **high-dimensional dataset**.

---

## ğŸ§ª Models & Performance

### ğŸ”¹ Models Evaluated
- **MLP Neural Network**
- **AdaBoost**
- **Gradient Boosting**

### ğŸ“ˆ Performance Comparison (Macro-F1)

| Model | Configuration | Macro-F1 |
|----|----|----|
| MLP | PCA-Reduced Features | 0.63 |
| MLP | Scaled Features (No PCA) | 0.64 |
| AdaBoost | Full Dataset (No Scaling) | 0.65 |
| **Gradient Boosting** â­ | Scaled Dataset | **0.68** |

---

## ğŸ† Why Gradient Boosting Won
- ğŸŒ² Tree-based ensembles handled **mixed feature types** effectively
- ğŸ” Boosting captured **non-linear relationships**
- ğŸ¤– MLP struggled especially with the **Enrolled** class
- âŒ PCA did not significantly improve neural network performance

> **Result:** Gradient Boosting outperformed MLP and AdaBoost, disproving the initial hypothesis that *MLP + PCA* would be the best model.

---

## ğŸš€ Practical Applications
This system can be used as an **early-warning tool** by educational institutions to:
- Identify at-risk students early
- Provide targeted academic support
- Offer counseling or financial assistance
- Improve retention and graduation rates

---

## ğŸ› ï¸ How to Run the Project

```bash
# 1. Download Data from UCI Machine Learning Repository

# 2. Launch Jupyter Notebook
```

## ğŸ“˜ Full Report
Detailed explanation with visualization and results are avaiable in the [ğŸ“„Full Report](ML_FINAL_REPORT_T00693024.pdf)
